import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, roc_curve
import matplotlib.pyplot as plt
import seaborn as sns

// Suppress warnings for cleaner output
import warnings
warnings.filterwarnings('ignore')

//📊 Data Generation

//Generate synthetic data for credit scoring
np.random.seed(42)
num_samples = 1000

data = {
    'income': np.random.normal(50000, 15000, num_samples),
    'debt': np.random.normal(10000, 5000, num_samples),
    'loan_amount': np.random.normal(20000, 8000, num_samples),
    'credit_card_utilization': np.random.uniform(0.1, 0.9, num_samples),
    'num_late_payments': np.random.randint(0, 5, num_samples),
    'credit_score_history': np.random.normal(700, 50, num_samples),
    'employment_length_years': np.random.randint(1, 20, num_samples)
}

df = pd.DataFrame(data)

//Ensure no negative values for income, debt, loan_amount
df['income'] = df['income'].apply(lambda x: max(10000, x))
df['debt'] = df['debt'].apply(lambda x: max(0, x))
df['loan_amount'] = df['loan_amount'].apply(lambda x: max(1000, x))
df['credit_score_history'] = df['credit_score_history'].apply(lambda x: max(300, min(850, x)))


//Create a target variable (creditworthiness)
//Assume 'good' credit if income is high, debt is low, few late payments, and high credit score history
df['creditworthiness'] = ((df['income'] / df['loan_amount'] > 2) &
                          (df['debt'] / df['income'] < 0.3) &
                          (df['num_late_payments'] < 2) &
                          (df['credit_card_utilization'] < 0.6) &
                          (df['credit_score_history'] > 650)).astype(int)

print("🔍 Sample Data Head:")
print(df.head())
print("\n")

print("🎯 Target Variable Distribution:")
print(df['creditworthiness'].value_counts())
print("\n")

// 🛠️ Feature Engineering
df['debt_to_income_ratio'] = df['debt'] / df['income']
df['loan_to_income_ratio'] = df['loan_amount'] / df['income']
df['payment_to_credit_ratio'] = df['num_late_payments'] / df['credit_score_history'] # Simplified ratio

print("📊 Data with Engineered Features Head:")
print(df.head())
print("\n")

//Define features (X) and target (y)
X = df.drop('creditworthiness', axis=1)
y = df['creditworthiness']

//Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)

print(f"Dataset split into training (X_train shape: {X_train.shape}, y_train shape: {y_train.shape}) and testing (X_test shape: {X_test.shape}, y_test shape: {y_test.shape}) sets.\n")

// 🚀 Model Training and Evaluation

// Initialize classifiers
classifiers = {
    'Logistic Regression': LogisticRegression(random_state=42),
    'Decision Tree': DecisionTreeClassifier(random_state=42),
    'Random Forest': RandomForestClassifier(random_state=42)
}

results = {}
roc_curves = {}

for name, clf in classifiers.items():
    print(f"🏋️ Training {name}...")
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    y_proba = clf.predict_proba(X_test)[:, 1] # Probability of positive class

    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    roc_auc = roc_auc_score(y_test, y_proba)
    fpr, tpr, _ = roc_curve(y_test, y_proba)

    results[name] = {
        'Precision': precision,
        'Recall': recall,
        'F1-Score': f1,
        'ROC-AUC': roc_auc
    }
    roc_curves[name] = {'fpr': fpr, 'tpr': tpr}

    print(f"✅ {name} Metrics:")
    print(f"   Precision: {precision:.4f}")
    print(f"   Recall: {recall:.4f}")
    print(f"   F1-Score: {f1:.4f}")
    print(f"   ROC-AUC: {roc_auc:.4f}\n")

## 📈 Results Summary and Visualization

print("✨ Model Performance Summary:")
for name, metrics in results.items():
    print(f"--- {name} ---")
    for metric, value in metrics.items():
        print(f"{metric}: {value:.4f}")
    print("\n")

// Plotting ROC Curves
plt.figure(figsize=(10, 7))
for name, curve in roc_curves.items():
    plt.plot(curve['fpr'], curve['tpr'], label=f'{name} (ROC-AUC = {results[name]["ROC-AUC"]:.2f})')
plt.plot([0, 1], [0, 1], 'k--', label='Random Guessing')
plt.xlabel('False Positive Rate (FPR)')
plt.ylabel('True Positive Rate (TPR)')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend()
plt.grid(True)
plt.show()

// Visualize feature importance for Random Forest
if 'Random Forest' in classifiers:
    rf_model = classifiers['Random Forest']
    feature_importances = pd.Series(rf_model.feature_importances_, index=X.columns).sort_values(ascending=False)

    plt.figure(figsize=(12, 7))
    sns.barplot(x=feature_importances.values, y=feature_importances.index, palette='viridis')
    plt.title('Random Forest Feature Importance')
    plt.xlabel('Importance')
    plt.ylabel('Feature')
    plt.show()
